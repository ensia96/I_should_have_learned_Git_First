스크랩핑(scraping) : 데이터를 수집하는 행위
크롤링(Crawling) : 조직적 자동화된 방법으로 월드와이드웹을 탐색 하는 것
파싱(parsing) : 문장 혹은 문서를 구성 성분으로 분해하고 위계관계를 분석하여 문장의 구조를 결정하는 것

BeautifulSoup 모듈은 HTML과 XML을 파싱하는 데에 사용되는 파이썬 라이브러리

사용자의 행동을 동적으로 추가하려면 selenium이 필요

페이지네이션 기술 - 스크롤할때마다 내용을 불러오는 방식

셀레니움 (selenium)

WebDriver : WebTesting 을 위해 웹브라우저를 구동해주는 프로그램
	Chrome WD : https://sites.google.com/a/chromium.org/chromedriver/downloads

	PhantomJS : https://phantomjs.org/download.html
		- Headless Browser, 화면 X / js 처리 다 해주고, CLI 환경에서도 사용가능 / Binary 자체로 제공


셀레니움의 웹드라이버 기능 호출
from selenium import webdriver

driver = webdriver.사용할WD('위치경로')

chrome
webdriver.Chrome('/Users/ensia96/Documents/chromedriver')

phantomjs
driver = webdriver.PhantomJS('/Users/ensia96/Documents/phantomjs/bin/phantomjs')

driver.implicitly_wait(3) # 웹 소스들이 모두 로드될 때까지 기다려주는 메소드 (시간)

driver.get("웹주소") # url 에 접근

페이지의 단일 element 에 접근하는 메소드 예시
( elements 로 바꿔주면 여러 개 실행 )

find_element_by_name('HTML_name')
find_element_by_id('HTML_id')
find_element_by_xpath('/html/body/some/xpath')
find_element_by_css_selector('#css > div.selector')
find_element_by_class_name('some_class_name')
find_element_by_tag_name('h1')

driver.page_source 라는 기능으로 element 를 모두 가져온 뒤, bs 로 작업하는게 나음 ( soup 객체가 더 사용이 더 편리하기 때문에 )



driver.page_source : 브라우저에 보이는 그대로의 HTML, 크롬 개발자 도구의 Element 탭 내용과 동일.

req.text : HTTP요청 결과로 받아온 HTML, 크롬 개발자 도구의 페이지 소스 내용과 동일.